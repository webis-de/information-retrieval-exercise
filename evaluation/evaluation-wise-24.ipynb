{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Lab WiSe 2024/2025: Evaluation\n",
    "\n",
    "This Jupyter notebook serves as an example on how to evaluate the research hypothesis that you developed throughout the course.\n",
    "\n",
    "We used a subset of the TREC 2019/2020 Deep Learning tracks on the MS MARCO v1 passage dataset for developing the systems and to formulate the research hypothesis, this dataset is loaded into the PyTerrier dataset `pt_training_dataset` below (having the dataset id `ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training`).\n",
    "\n",
    "Furthermore, we developed an own test dataset together throughout the course on the MS MARCO v2.1 passage dataset. You can use this dataset to test if your research hypothesis was true on an unseen evaluation corpus. This dataset is loaded into the PyTerrier dataset `pt_test_dataset` below (having the dataset id `ir-lab-wise-2024/subsampled-ms-marco-ir-lab-20250105-test`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Ensure Libraries are installed\n",
    "\n",
    "We install the dependencies and clean up old versions of the test dataset that had no qrels available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install 'tira>=0.0.141' ir-datasets 'python-terrier==0.10.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cached versions of the dataset that might not have the qrels\n",
    "!rm -Rf ~/.tira/extracted_datasets/ir-lab-wise-2024/subsampled-ms-marco-ir-lab-20250105-test\n",
    "!rm -Rf ~/.tira/.archived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Dependencies and Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client\n",
    "from ir_measures import nDCG, Judged, RR\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "from pyterrier import get_dataset, Experiment\n",
    "\n",
    "pt_training_dataset = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training')\n",
    "pt_test_dataset = get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-ir-lab-20250105-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the runs for the Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_TEAM = 'ir-wise-24-tutors'\n",
    "MY_APPROACH = 'Retrieval Baseline'\n",
    "\n",
    "trainings_run = tira.pt.from_retriever_submission(f'ir-lab-wise-2024/{MY_TEAM}/{MY_APPROACH}', dataset=pt_training_dataset)\n",
    "\n",
    "test_run = tira.pt.from_retriever_submission(f'ir-lab-wise-2024/{MY_TEAM}/{MY_APPROACH}', dataset=pt_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation on the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>nDCG(judged_only=True)@10</th>\n",
       "      <th>RR@10</th>\n",
       "      <th>Judged@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Retrieval Baseline</td>\n",
       "      <td>0.489469</td>\n",
       "      <td>0.495665</td>\n",
       "      <td>0.784737</td>\n",
       "      <td>0.94433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name   nDCG@10  nDCG(judged_only=True)@10     RR@10  \\\n",
       "0  Retrieval Baseline  0.489469                   0.495665  0.784737   \n",
       "\n",
       "   Judged@10  \n",
       "0    0.94433  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment(\n",
    "    retr_systems = [trainings_run],\n",
    "    names = [MY_APPROACH],\n",
    "    eval_metrics = [nDCG@10, nDCG(judged_only=True)@10, RR@10, Judged@10],\n",
    "    topics = pt_training_dataset.get_topics('title'),\n",
    "    qrels = pt_training_dataset.get_qrels(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluation on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>nDCG(judged_only=True)@10</th>\n",
       "      <th>RR@10</th>\n",
       "      <th>Judged@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Retrieval Baseline</td>\n",
       "      <td>0.493561</td>\n",
       "      <td>0.542444</td>\n",
       "      <td>0.624715</td>\n",
       "      <td>0.908696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name   nDCG@10  nDCG(judged_only=True)@10     RR@10  \\\n",
       "0  Retrieval Baseline  0.493561                   0.542444  0.624715   \n",
       "\n",
       "   Judged@10  \n",
       "0   0.908696  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment(\n",
    "    retr_systems = [test_run],\n",
    "    names = [MY_APPROACH],\n",
    "    eval_metrics = [nDCG@10, nDCG(judged_only=True)@10, RR@10, Judged@10],\n",
    "    topics = pt_test_dataset.get_topics('title'),\n",
    "    qrels = pt_test_dataset.get_qrels(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
